{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64eca1d-8325-4896-a6eb-fb348d953746",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e05430-d65e-4b49-9d77-cf097a38f663",
   "metadata": {},
   "source": [
    "This notebook applies a workflow that is built on [faster-whisper](https://github.com/SYSTRAN/faster-whisper) and [pyannote.audio](https://github.com/pyannote/pyannote-audio), with installation procedures based on their official GitHub repositories (accessed September 25, 2025). This workflow is one common Whisper-based workflow among several: Whisper can also be run directly (without faster-whisper) or via more integrated pipelines such as [WhisperX](https://github.com/m-bain/whisperX). \n",
    "\n",
    "The overall approach that we present in this notebook is: \n",
    "1. **Automatic Speech Recognition (ASR)** with Whisper (speech-to-text transcription)\n",
    "2. **Speaker diarization** with pyannote (who speaks when)\n",
    "3. **Time-based merging** (assign speaker labels to the transcript)\n",
    "\n",
    "This is similar to what runs in the background of tools like [noScribe](https://github.com/kaixxx/noScribe). \n",
    "\n",
    "If you only need a plain transcript (no speaker labels), you can run Whisper alone and skip pyannote (including the Hugging Face setup) as well as the merging steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79bc53-5394-4516-82c3-1c7c498eb7c9",
   "metadata": {},
   "source": [
    "# 1. One-Time Setup: Install Software & Hugging Face Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad2619-4d9b-4d73-9885-fb8fc7b86991",
   "metadata": {},
   "source": [
    "## 1.1 Install Software\n",
    "\n",
    "The code below installs the **Python packages** listed in \"requirements.txt\" into the environment your Jupyter notebook is using. It ensures all needed libraries (and versions) are available so the notebook can run without import errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0df04-6048-4ce5-98c2-cc753f6c0a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed07d1-1344-4c9f-b755-577854e35490",
   "metadata": {},
   "source": [
    "## 1.2 Hugging Face Account\n",
    "\n",
    "Pyannote diarization requires a Hugging Face account and model access. The setup consists of three steps:\n",
    "1. Create a **Hugging Face account** (if you don’t have one yet): [Hugging Face Account](https://huggingface.co/)\n",
    "2. Create an **access token** (needed to download and run the diarization model):\n",
    "Click your profile icon -> Settings -> Access Tokens → create a new token with read permission (no write permission needed; easiest: select \"Read\" token type).\n",
    "\n",
    ">Important: Don't share your token.\n",
    "\n",
    "3. **Request access** to the model repository: Open [pyannote/speaker-diarization-community-1](https://huggingface.co/pyannote/speaker-diarization-community-1), review the conditions, enter your information if required, and click \"Agree and access repository\".\n",
    "\n",
    "If you encounter any error messages when applying speaker diarisation to this notebook due to changes in the steps or requirements, please check the latest instructions in the [pyannote.audio](https://github.com/pyannote/pyannote-audio) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb78090-1ec5-44eb-a23a-c43a30ff419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_token = \"ENTER_YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df78052-e7cc-480d-a742-ba2b62ca189f",
   "metadata": {},
   "source": [
    "# 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceb4304-d329-4275-bef7-afbb3e9fcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional to avoid warnings:\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07700c-022c-4e81-aac2-1695b3e0f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import subprocess\n",
    "\n",
    "import imageio_ffmpeg          # ffmpeg is an external program; imageio_ffmpeg provides an executable we can call from Python for audio conversion\n",
    "import torch                   # PyTorch: required by pyannote and used for tensors / GPU support\n",
    "\n",
    "from faster_whisper import WhisperModel              # speech-to-text (ASR) model\n",
    "from faster_whisper import BatchedInferencePipeline  # optional wrapper for faster ASR on GPU (batching audio chunks)\n",
    "\n",
    "import torchaudio                    # used in this notebook to load audio into memory as a waveform (tensor) + sample rate\n",
    "from pyannote.audio import Pipeline  # speaker diarization pipeline (\"who speaks when\")\n",
    "\n",
    "# Path to the ffmpeg executable (used in subprocess calls for conversion)\n",
    "ffmpeg = imageio_ffmpeg.get_ffmpeg_exe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d74e8-cee9-459d-8d6e-d88bc839dfe9",
   "metadata": {},
   "source": [
    "You can check the installed PyTorch version and whether your environment has access to a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789cca8-dc80-43b6-9646-db8d6ce158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c899e-4f39-4401-8c74-af81d48de1bc",
   "metadata": {},
   "source": [
    "# 3. Setup\n",
    "\n",
    "## 3.1 Runtime Setup\n",
    "\n",
    "Automatically set device and compute type depending on **hardware availability**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4baa74-0c13-4507-a9ca-cb5efe98c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    compute_type = \"float16\"  # Faster and more memory efficient on GPU\n",
    "    batch_size = 16           # Adjust based on GPU memory\n",
    "else:\n",
    "    compute_type = \"int8\"     # Required or more efficient on CPU\n",
    "    batch_size = 1            # Keep at 1 on CPU (larger values don’t help and may cause memory issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569593c-ede0-4a8a-8020-3d7b55671224",
   "metadata": {},
   "source": [
    "## 3.2 Select Audio File\n",
    "\n",
    "With Python, you can easily transcribe **multiple files by looping over a list of paths** (e.g., all files in a folder) and applying the same steps to each file. In this notebook, however, we keep things simple and specify a single audio file. \n",
    "\n",
    "Below, we provide the **relative path to one audio file** (going up one folder into, e.g., \"Data/buffy/\"). Both .wav and .mp3 files work because the transcription library uses ffmpeg under the hood to read many common audio formats. In addition, in the next step we explicitly convert the audio to a standardized format to ensure consistent processing throughout the notebook. \n",
    "\n",
    "To switch between example files, uncomment exactly one pair (`file_name` & `audio_file`) and keep all others commented out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e63aa-771e-4a33-b8cb-6a4aebdcad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"File-A\"\n",
    "#audio_file = \"../Data_Raw/File-A_buffy/shortened_Buffy_Seas01-Epis01.en.wav\"\n",
    "#file_name = \"File-B\"\n",
    "#audio_file = \"../Data_Raw/File-B_moon-landing/CA138clip.mp3\"\n",
    "file_name = \"File-C\"\n",
    "audio_file = \"../Data_Raw/File-C_qualitative-interview-de/DE_example_2.mp3\"\n",
    "#file_name = \"File-D\"\n",
    "#audio_file = \"../Data_Raw/File-D_qualitative-interview-en/EN_example_1.mp3\"\n",
    "#file_name = \"File-E\"\n",
    "#audio_file = \"../Data_Raw/File-E_Bremen-guide-low-saxon/audioguide-2025-platt-01-shortened.wav\"\n",
    "#file_name = \"File-F\"\n",
    "#audio_file = \"../Data_Raw/File-F_math/math4.mp3\"\n",
    "#file_name = \"File-G\"\n",
    "#audio_file = \"../Data_Raw/File-G_XX/XX\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb9d09",
   "metadata": {},
   "source": [
    "# 4. Preprocess Audio File\n",
    "\n",
    "Whisper and pyannote can read many audio formats and handle basic resampling internally. In this notebook, we still apply one light **preprocessing** step: we standardize the audio to a 16 kHz mono WAV. This ensures that Whisper and pyannote use the exact same audio file and time base, which makes the later alignment/merging step more reliable (see Sect. 8).\n",
    "\n",
    "More advanced preprocessing (denoising, volume normalization, echo removal, speech separation) is usually optional and only needed if you notice clear problems, such as strong background noise/echo, very uneven volume, very long silences, or heavy overlapping speech.\n",
    "\n",
    "## 4.1 Run ffmpeg\n",
    "\n",
    "The audio file is converted once to a 16 kHz mono WAV. This **standardized audio file** is then used for both Whisper and pyannote so they share the exact same audio time base. This makes it more straightforward to align Whisper’s transcript segments with pyannote’s speaker turns and assign speaker labels (see Sect. 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_16k = \"../Data_Preprocessed/audio_16k_mono.wav\"\n",
    "\n",
    "# Convert with ffmpeg (standardize audio for consistent processing):\n",
    "# -i <input> -> input audio file (e.g., .mp3 or .wav)\n",
    "# -ac 1      -> convert to mono (1 audio channel)\n",
    "# -ar 16000  -> resample to 16,000 Hz (common format for speech models)\n",
    "# -y         -> overwrite output file if it already exists\n",
    "# -hide_banner       -> hide ffmpeg version banner\n",
    "# -loglevel error    -> show only errors (no progress/info output)\n",
    "subprocess.run(\n",
    "    [ffmpeg, \"-y\",\n",
    "     \"-hide_banner\",\n",
    "     \"-loglevel\", \"error\",\n",
    "     \"-i\", audio_file,\n",
    "     \"-ac\", \"1\", \"-ar\", \"16000\",\n",
    "     audio_16k],\n",
    "    check=True\n",
    ")\n",
    "\n",
    "print(\"Wrote:\", audio_16k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263afb6-ca29-4d0e-936e-056a1229f424",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 5. Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b53474",
   "metadata": {},
   "source": [
    "## 5.1 Load Whisper Model for ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083761cc-f876-426d-9533-3063d2d21318",
   "metadata": {},
   "source": [
    "Load the **Whisper model for ASR** with the given device (\"cpu\" or \"cuda\") and precision type (\"float16\", \"int8\", etc.). You can select any [Whisper model](https://github.com/openai/whisper) size (e.g., \"tiny\" to \"large-v3\") or provide a custom/fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982a27-9bd6-4e96-a0c0-291ae0af4be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = WhisperModel(\"large-v3\",  # \"tiny\", ...\n",
    "                     device, \n",
    "                     compute_type=compute_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316311f-94f3-4fe5-8266-83220fbce9d4",
   "metadata": {},
   "source": [
    "With faster-whisper, you can run transcription \"normally\" or with batched inference. Batched inference is mainly a speed option for GPUs (it processes several audio chunks at once). On CPU, it usually provides little benefit, so the default (non-batched) mode is typically used. If you want the batched model (alternative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d2651-225c-4744-a96b-4eff3312f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BatchedInferencePipeline(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a571bbc",
   "metadata": {},
   "source": [
    "## 5.2 Load pyannote Model for Diarization\n",
    "\n",
    "Load the **pyannote speaker diarization pipeline from Hugging Face** using your access token. This model will later predict a timeline of \"who speaks when\" in the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a37d2-1d93-40f0-8c4f-b1609f25d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    token=own_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4be45-c508-4e27-bb17-1b808f2d02a1",
   "metadata": {},
   "source": [
    "# 6. Run Transcription / ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03298dd2-22f6-4259-a5a7-65e0228dbe74",
   "metadata": {},
   "source": [
    "The `transcribe()` function takes an audio input and produces a **transcription as a sequence of time-stamped text segments.**\n",
    "\n",
    "You can optionally provide a **language code**. It is one of **many optional settings**. Most of the other parameters control how the decoding is done: \n",
    "\n",
    "- In the setup below, you can enable or disable **Word-Level timestamps** (`word_timestamps`). If enabled, per-word start/end times and word probabilities are added, but compute cost increases and timestamps can be less stable for short disfluencies or noisy speech.\n",
    "- You can also enable or disable **voice activity detection** (`vad_filter`). If enabled, non-speech regions are removed, which typically speeds up transcription and can reduce hallucinations in silent/noisy parts, but it may also cut very short hesitation sounds (e.g., “um”, “ähm”) if they fall below the VAD sensitivity.\n",
    "- **Beam size** (`beam_size`) controls how many alternative token sequences are considered during decoding. Larger values can slightly improve accuracy, but they also increase runtime.\n",
    "\n",
    "For details on all available parameters and their defaults, refer to the [faster-whisper](https://github.com/SYSTRAN/faster-whisper) documentation.\n",
    "\n",
    ">Example: \n",
    ">\n",
    ">In [noScribe](https://github.com/kaixxx/noScribe), **hotwords** from a separate file are passed into `transcribe()` via the `hotwords` parameter to implement the \"disfluencies\" on/off toggle in the noScribe application. In faster-whisper, these hotwords are inserted as extra prompt tokens before decoding, which slightly biases the decoder toward producing those words when the audio is uncertain (for example, very short filler sounds like \"uh\" or \"ehm\" that can be hard to distinguish from breathing or background noise). The original OpenAI [Whisper](https://github.com/openai/whisper) implementation does not provide a `hotwords` parameter under that name, so this behavior is specific to faster-whisper. The closest equivalent is providing a prompt via the `initial_prompt` parameter to bias decoding in a similar direction. \n",
    ">\n",
    ">If you want to find out how accounting for hesitation sounds could be implemented in the `transcribe()` function below, add to the parameters `hotwords = \"Äh, das ist, es ist, ähm, nicht so einfach.\"` or `hotwords = \"Uhm, okay, here's what I'm, like, thinking.\"` (taken from noScribe`s [prompt.yml](https://github.com/kaixxx/noScribe/blob/main/prompt.yml)) and run the transcription with the audio file \"C\" (german example) or \"D\" (english example) that you can select in Sect. 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70298242-ef91-4046-a019-956932ce1ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\n",
    "    audio_16k,\n",
    "    language=\"de\",\n",
    "    word_timestamps=False, \n",
    "    vad_filter=True, \n",
    "    beam_size=5\n",
    ")\n",
    "\n",
    "segments = list(segments)  # The transcription will actually run here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d41380",
   "metadata": {},
   "source": [
    "In faster-whisper **transcription results**, each text **segment** includes:\n",
    "- start and end time, \n",
    "- recognized text,\n",
    "- the underlying token IDs,\n",
    "- and several scores that can be inspected if needed. These scores are model-internal confidence signals derived from the token probabilities during decoding.\n",
    "- If word-level timestamps are enabled, each segment also contains a list of words with their own start and end times and (model-derived) probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segments) # Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a817a8-e86f-4885-9f20-edb7ac658e9e",
   "metadata": {},
   "source": [
    "# 7. Run Speaker Diarization\n",
    "\n",
    "Next, we run the **diarization model** on the audio file. This gives us a **timeline of who speaks when**, for example: *SPEAKER_00 speaks from 0–10 seconds, then SPEAKER_01 speaks from 10–15 seconds, and so on.* By setting `min_speakers` and `max_speakers`, we constrain the output to exactly that **number of speaker labels**, even if the real audio might contain fewer or more speakers.\n",
    "\n",
    "In Sect. 4, `imageio_ffmpeg` provides an ffmpeg executable that we call directly to convert audio files. pyannote, however, loads audio files via its own internal decoder (`TorchCodec`; see [pyannote.audio](https://github.com/pyannote/pyannote-audio)). This decoder can fail on some machines even when ffmpeg itself works. To avoid this, we pass audio \"from memory\", meaning we first load the audio into Python (RAM) as a waveform (the raw audio samples as a numeric array) plus the sample rate (samples per second), and then pass these values directly to the diarization model. This makes the notebook more reliable on Windows and on JupyterHubs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1af5d-34d0-4e3b-8a82-105ec8dd5980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_for_pyannote(path):\n",
    "    \"\"\"Load audio into memory (RAM) and output waveform shape: (channels, samples).\"\"\"\n",
    "    waveform, sr = torchaudio.load(path)\n",
    "    return {\"waveform\": waveform, \"sample_rate\": sr}\n",
    "\n",
    "# Run speaker diarization on the audio:\n",
    "diarization_result = diarization(\n",
    "    load_for_pyannote(audio_16k),\n",
    "    min_speakers=2,\n",
    "    max_speakers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c996dce-a3a3-4b42-89ca-a7652610a7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(diarization_result.speaker_diarization) # Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d24bb8-6174-49b6-a12a-b329b89d4880",
   "metadata": {},
   "source": [
    "# 8. Merge Results\n",
    "\n",
    "We merge ASR output with diarization results so that **each spoken segment (and optionally each word) is linked to a speaker label** (e.g., SPEAKER_00). If word-level timestamps are available, we assign speakers per word; otherwise we assign one speaker per segment. \n",
    "\n",
    "The code in this section illustrates a **manual preparation** of transcription and diarization outputs and merging strategy and **can be adapted** if requirements differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d469faf-3e54-48fb-90d8-faa22c9ff912",
   "metadata": {},
   "source": [
    "## 8.1 ASR Result Formatting\n",
    "\n",
    "Since the output of faster-whisper is stored in its own custom Python objects, we first convert it into a simple **Python dictionary**. We define a function `to_whisper_result` that extracts only the fields we need. This makes the transcript easier to inspect, save, and process in later steps. Each segment gets a start time, an end time, the transcribed text, and (if word-level timestamps were enabled) a list of words with their own timing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b8cd3-85b7-4472-ba2a-1fc7bb7c9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_whisper_result(segments):\n",
    "    \"\"\"Convert faster-whisper segment objects into a Python dictionary.\"\"\"\n",
    "    out = {\"segments\": []}\n",
    "    for s in segments:\n",
    "        item = {\n",
    "            \"start\": float(s.start),\n",
    "            \"end\": float(s.end),\n",
    "            \"text\": s.text or \"\",\n",
    "        }\n",
    "        # Add word-level timestamps if they exist (word_timestamps=True)\n",
    "        if getattr(s, \"words\", None):\n",
    "            item[\"words\"] = [\n",
    "                {\"word\": w.word, \"start\": float(w.start), \"end\": float(w.end)}\n",
    "                for w in s.words\n",
    "                if w.start is not None and w.end is not None\n",
    "            ]\n",
    "        out[\"segments\"].append(item)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba196bc-7fa4-416e-b04c-15ddd9baa428",
   "metadata": {},
   "source": [
    "Run conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d8395-4a3d-485f-94d3-9ac82f53f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_result = to_whisper_result(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4590e0e-e1ee-43e1-ac54-032c3f48be55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(asr_result) # Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cc425-e101-45d8-8b01-3fc0e8204159",
   "metadata": {},
   "source": [
    "## 8.2 Diarization Result Formatting\n",
    "\n",
    "The pyannote diarization output is also stored in a custom object format. Before merging, we convert it into a simple **list of speaker time intervals**. Each entry contains:\n",
    "- start time\n",
    "- end time\n",
    "- speaker label\n",
    "\n",
    "This makes the overlap comparison with the transcript easier and avoids repeatedly iterating over the pyannote object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5ec5a-7116-46f7-8957-481a53c851ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "turns = []\n",
    "for turn, _, spk in diarization_result.itertracks(yield_label=True):\n",
    "    turns.append((float(turn.start), float(turn.end), spk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffff5e0e-32a6-4ee0-92d6-b5b50ec13b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(turns) # Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf850a-9e20-4308-87ea-64d432fbd16b",
   "metadata": {},
   "source": [
    "## 8.3 Align Results\n",
    "\n",
    "We **combine the converted ASR output with the diarization output** in the `align` function. For each ASR segment (and for each word, if word-level timestamps are available), we:\n",
    "- assign speaker labels: per word if word-level timestamps are available; and always a single main speaker for each ASR segment (based on total word duration or, if no words are available, based on overlap with the segment interval).\n",
    "- compute an overlap flag (True when two or more different speakers are active at any point within the interval).\n",
    "\n",
    "The alignment shown here is an example and can be adapted to different needs. For example, you could merge adjacent segments with the same speaker into longer turns, or change the decision rule (e.g., using total duration vs. word count).\n",
    "\n",
    ">Important: Diarization segments and ASR segments are independent time partitions, so they rarely match perfectly. The merge therefore compares time intervals and links them based on temporal overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca2c4b-28cb-4d79-8b83-6c1adb7de41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(asr_result, turns):\n",
    "    \"\"\"\n",
    "    Merge ASR output with diarization turns.\n",
    "\n",
    "    We work with time intervals:\n",
    "    - ASR segments/words: [start (s), end (e)]\n",
    "    - diarization speaker turns: [turn_start (ts), turn_end (te)]\n",
    "    \"\"\"\n",
    "\n",
    "    def overlap_seconds(s, e, ts, te):\n",
    "        \"\"\"Return overlap duration (in seconds) between [s, e] and [ts, te].\"\"\"\n",
    "        return max(0.0, min(e, te) - max(s, ts))\n",
    "\n",
    "    def best_speaker(s, e):\n",
    "        \"\"\"Return the speaker with the largest total overlap with [s, e].\"\"\"\n",
    "        overlap_by_speaker = {}\n",
    "        for ts, te, spk in turns:\n",
    "            ov = overlap_seconds(s, e, ts, te)\n",
    "            if ov > 0:\n",
    "                overlap_by_speaker[spk] = overlap_by_speaker.get(spk, 0.0) + ov\n",
    "        return max(overlap_by_speaker, key=overlap_by_speaker.get) if overlap_by_speaker else None\n",
    "\n",
    "    def is_overlapped(s, e):\n",
    "        \"\"\"True if >= 2 different speakers overlap with [s, e].\"\"\"\n",
    "        seen = set()\n",
    "        for ts, te, spk in turns:\n",
    "            if overlap_seconds(s, e, ts, te) > 0:\n",
    "                seen.add(spk)\n",
    "                if len(seen) >= 2:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    # Output structure: a dict with a list of segments\n",
    "    out = {\"segments\": []}\n",
    "\n",
    "    # Loop over each ASR segment\n",
    "    for seg in asr_result[\"segments\"]:\n",
    "\n",
    "        # 1) Basic info: start/end time and full text\n",
    "        new_seg = {\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"text\": seg[\"text\"],\n",
    "        }\n",
    "\n",
    "        # 2) Segment-level overlap flag\n",
    "        new_seg[\"overlap\"] = is_overlapped(seg[\"start\"], seg[\"end\"])\n",
    "\n",
    "        # 3) Speaker labels\n",
    "        # ---- only needed if your ASR output contains word-level timestamps (seg[\"words\"])\n",
    "        #      (i.e., you want per-word speaker labels + per-word overlap flags and want\n",
    "        #      to derive the segment speaker from summed word durations):\n",
    "        words = seg.get(\"words\", [])\n",
    "        if words:\n",
    "            new_words = []\n",
    "            dur_by_speaker = {}  # total word duration per speaker within this ASR segment\n",
    "\n",
    "            for w in words:\n",
    "                # Speaker for this word based on maximum time overlap\n",
    "                spk = best_speaker(w[\"start\"], w[\"end\"])\n",
    "\n",
    "                # Copy word info and add speaker + overlap flag\n",
    "                wd = dict(w)\n",
    "                wd[\"speaker\"] = spk\n",
    "                wd[\"overlap\"] = is_overlapped(w[\"start\"], w[\"end\"])\n",
    "                new_words.append(wd)\n",
    "\n",
    "                # Accumulate speaking time (word duration) per speaker\n",
    "                if spk is not None:\n",
    "                    dur_by_speaker[spk] = dur_by_speaker.get(spk, 0.0) + (w[\"end\"] - w[\"start\"])\n",
    "\n",
    "            # Save the word list inside the segment\n",
    "            new_seg[\"words\"] = new_words\n",
    "\n",
    "            # Segment-level main speaker: pick the one with the most total word duration\n",
    "            new_seg[\"speaker\"] = max(dur_by_speaker, key=dur_by_speaker.get) if dur_by_speaker else None\n",
    "        # ---- else (no word-level timestamps): segment-only speaker assignment:\n",
    "        else:\n",
    "            # If no word-level timestamps exist, assign a speaker for the entire segment directly\n",
    "            new_seg[\"speaker\"] = best_speaker(seg[\"start\"], seg[\"end\"])\n",
    "\n",
    "        out[\"segments\"].append(new_seg)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f050459-d392-4d89-8510-a64f6d7c1fcf",
   "metadata": {},
   "source": [
    "Run aligner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a51ad-c09e-476a-bcd0-be0283095bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = align(asr_result, turns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e97df-8c2d-4aa6-a0fe-073ce9f9cfe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(final_result) # Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265cf52-c955-4384-8782-1e7af40fd84f",
   "metadata": {},
   "source": [
    "# 9. Save Transcript\n",
    "\n",
    "Finally, we save a **readable transcript as a text file**. For each segment, we write the speaker label, the transcribed text, and the segment start time. If a segment overlaps with another speaker, we add a tag. This produces a *SPEAKER: text [OVERLAP] [time]* format.\n",
    "\n",
    "You **can adapt the format** depending on what you need for analysis in Python or external tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab62bb-cd1b-420e-bb9c-ab9f7f38e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as ...\n",
    "output_folder = \"../Results/\"\n",
    "txt_path = os.path.join(output_folder, f\"{file_name}.txt\")\n",
    "\n",
    "# Save \n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for seg in final_result[\"segments\"]:\n",
    "        speaker = seg.get(\"speaker\")\n",
    "        text = seg[\"text\"].strip()\n",
    "        start = str(timedelta(seconds=seg[\"start\"]))[:-3]\n",
    "        overlap_flag = \" [OVERLAP]\" # if seg.get(\"overlap\") else \"\"\n",
    "        f.write(f\"{speaker}: {text}{overlap_flag} [{start}]\\n\\n\")\n",
    "\n",
    "print(\"Saved transcript to\", txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaba393",
   "metadata": {},
   "source": [
    "If you would only run Whisper transcription (without diarization), you can save the result directly like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as ...\n",
    "#output_folder = \"../Results/\"\n",
    "#os.makedirs(output_folder, exist_ok=True)\n",
    "#txt_path = os.path.join(output_folder, f\"{file_name}.txt\")\n",
    "\n",
    "# Save\n",
    "#with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#    for s in segments:\n",
    "#        start = str(timedelta(seconds=float(s.start)))[:-3]\n",
    "#        f.write(f\"[{start}] {s.text.strip()}\\n\")\n",
    "\n",
    "#print(\"Saved transcript to\", txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71af32d-5098-4e6e-9db8-f3824814de31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
