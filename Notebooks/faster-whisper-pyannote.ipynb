{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64eca1d-8325-4896-a6eb-fb348d953746",
   "metadata": {},
   "source": [
    "<img src=\"../Images/DSC_Logo.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e05430-d65e-4b49-9d77-cf097a38f663",
   "metadata": {},
   "source": [
    "This notebook applies a pipeline that is built on [faster-whisper](https://github.com/SYSTRAN/faster-whisper) and [pyannote.audio](https://github.com/pyannote/pyannote-audio), with installation procedures based on their official GitHub repositories (accessed September 25, 2025). This is one common Whisper-based workflow among several: Whisper can also be run directly (without faster-whisper) or via more integrated pipelines such as [WhisperX](https://github.com/m-bain/whisperX). The overall approach is: Whisper transcription + pyannote diarization + time-based merging. This is similar to tools like [noScribe](https://github.com/kaixxx/noScribe). If you only need a plain transcript (no speaker labels), you can run Whisper alone and skip pyannote (including Hugging Face setup), and the merging steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79bc53-5394-4516-82c3-1c7c498eb7c9",
   "metadata": {},
   "source": [
    "# 1. One-Time Setup: Install Software & Hugging Face Account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad2619-4d9b-4d73-9885-fb8fc7b86991",
   "metadata": {},
   "source": [
    "## 1.1 Install Software\n",
    "\n",
    "This code installs the Python packages listed in \"requirements.txt\" into the environment your Jupyter notebook is using. It ensures all needed libraries (and versions) are available so the notebook can run without import errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa0df04-6048-4ce5-98c2-cc753f6c0a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ed07d1-1344-4c9f-b755-577854e35490",
   "metadata": {},
   "source": [
    "## 1.2 Hugging Face Account\n",
    "\n",
    "If you don't have a [Hugging Face Account](https://huggingface.co/) you need to create one. \n",
    "\n",
    "You will require token later on for the speaker diarization. You can create one by clicking on your profile icon; next click on \"Access Tokens\": Create a new access token (\"read\" permission but no \"write\" permissions needed). \n",
    "\n",
    ">Important: Don't share your token.\n",
    "\n",
    "In addition, pyannote requires you to agree to share your contact information to access it's models. For that, go on the [pyannote/speaker-diarization-community-1](https://huggingface.co/pyannote/speaker-diarization-community-1) page, enter your information, and click on \"Agree and access repository\". Please also have a look at [pyannote.audio](https://github.com/pyannote/pyannote-audio) in case anything changes with the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb78090-1ec5-44eb-a23a-c43a30ff419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "own_token = \"ENTER_YOUR_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df78052-e7cc-480d-a742-ba2b62ca189f",
   "metadata": {},
   "source": [
    "# 2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07700c-022c-4e81-aac2-1695b3e0f415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import subprocess\n",
    "\n",
    "import imageio_ffmpeg\n",
    "import soundfile as sf\n",
    "import torch\n",
    "\n",
    "from faster_whisper import WhisperModel # faster-whisper\n",
    "from faster_whisper import BatchedInferencePipeline # Optional: can make transcription faster (especially on a GPU) by processing several audio chunks at once\n",
    "\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline # pyannote: a wrapper that gives ready-to-use model for speaker diarization task\n",
    "\n",
    "FFMPEG = imageio_ffmpeg.get_ffmpeg_exe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d74e8-cee9-459d-8d6e-d88bc839dfe9",
   "metadata": {},
   "source": [
    "You can check the installed PyTorch version and whether your environment has access to a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789cca8-dc80-43b6-9646-db8d6ce158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c899e-4f39-4401-8c74-af81d48de1bc",
   "metadata": {},
   "source": [
    "# 3. Setup\n",
    "\n",
    "## 3.1 Runtime Setup\n",
    "\n",
    "Automatically set device and compute type depending on hardware availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4baa74-0c13-4507-a9ca-cb5efe98c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if device == \"cuda\":\n",
    "    compute_type = \"float16\"  # Faster and more memory efficient on GPU\n",
    "    batch_size = 16           # Adjust based on GPU memory\n",
    "else:\n",
    "    compute_type = \"int8\"     # Required or more efficient on CPU\n",
    "    batch_size = 1            # Keep at 1 on CPU (larger values don’t help and may cause memory issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569593c-ede0-4a8a-8020-3d7b55671224",
   "metadata": {},
   "source": [
    "## 3.2 Select Audio File\n",
    "\n",
    "With Python, you can easily transcribe multiple files by looping over a list of paths (e.g., all files in a folder) and applying the same steps to each file. In this notebook, however, we keep things simple and specify a single audio file. Provide the relative path to one audio file (going up one folder into, e.g., \"Data/buffy/\"). Both .wav and .mp3 files work because the transcription library uses ffmpeg under the hood to read many common audio formats. In addition, in the next step we explicitly convert the audio to a standardized format to ensure consistent processing.\n",
    "\n",
    "To switch between example files, uncomment exactly one pair (file_name & audio_file) and keep all others commented out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e63aa-771e-4a33-b8cb-6a4aebdcad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"File-A\"\n",
    "#audio_file = \"../Data_Raw/File-A_buffy/shortened_Buffy_Seas01-Epis01.en.wav\"\n",
    "\n",
    "#file_name = \"File-B\"\n",
    "#audio_file = \"../Data_Raw/File-B_moon-landing/CA138clip.mp3\"\n",
    "\n",
    "file_name = \"File-C\"\n",
    "audio_file = \"../Data_Raw/File-C_qualitative-interview-de/DE_example_2.mp3\"\n",
    "\n",
    "#file_name = \"File-D\"\n",
    "#audio_file = \"../Data_Raw/File-D_qualitative-interview-en/EN_example_1.mp3\"\n",
    "\n",
    "#file_name = \"File-E\"\n",
    "#audio_file = \"../Data_Raw/File-E_Bremen-guide-low-saxon/audioguide-2025-platt-01-shortened.wav\"\n",
    "\n",
    "#file_name = \"File-F\"\n",
    "#audio_file = \"../Data_Raw/File-F_math/math4.mp3\"\n",
    "\n",
    "#file_name = \"File-G\"\n",
    "#audio_file = \"../Data_Raw/File-G_XX/XX\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb9d09",
   "metadata": {},
   "source": [
    "# 4. Preprocess Audio File\n",
    "\n",
    "Whisper and pyannote already do the basic audio handling (decoding/resampling and speech/non-speech detection), so extra preprocessing is usually optional. You would only add it if you notice clear problems, for example strong background noise/echo, very uneven volume, very long silences, or heavy overlapping speech (where an advanced \"speech separation\" step can sometimes help).\n",
    "\n",
    "## 4.1 Run ffmpeg\n",
    "The audio file is once converted to a 16 kHz mono WAV. This file is then used for both Whisper and pyannote so they share the exact same audio time base. The preprocessing step makes it more straightforward to align Whisper's transcript segments with pyannote's speaker turns and assign a speaker label to each part of the transcript (see Sect. 8.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_16k = \"../Data_Preprocessed/audio_16k_mono.wav\"\n",
    "\n",
    "subprocess.run([FFMPEG, \"-y\", \"-i\", audio_file, \"-ac\", \"1\", \"-ar\", \"16000\", audio_16k],\n",
    "               check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263afb6-ca29-4d0e-936e-056a1229f424",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 5. Load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b53474",
   "metadata": {},
   "source": [
    "## 5.1 Load Whisper Model (ASR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083761cc-f876-426d-9533-3063d2d21318",
   "metadata": {},
   "source": [
    "Load the model with the given device (\"cpu\" or \"cuda\") and precision type (\"float16\", \"int8\", etc.). Refer to [Whisper](https://github.com/openai/whisper) or use a custom (e.g. fine-tuned) model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982a27-9bd6-4e96-a0c0-291ae0af4be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = WhisperModel(\"large-v3\", \n",
    "                     device, \n",
    "                     compute_type=compute_type) # \"tiny\", ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316311f-94f3-4fe5-8266-83220fbce9d4",
   "metadata": {},
   "source": [
    "With faster-whisper, you can run transcription \"normally\" or with batched inference. Batched inference is mainly a speed option for GPUs (it processes several audio chunks at once). On CPU, it usually provides little benefit, so the default (non-batched) mode is typically used. If you want the batched model (alternative):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d2651-225c-4744-a96b-4eff3312f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BatchedInferencePipeline(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a571bbc",
   "metadata": {},
   "source": [
    "## 5.2 Load pyannote Diarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a37d2-1d93-40f0-8c4f-b1609f25d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    token=own_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4be45-c508-4e27-bb17-1b808f2d02a1",
   "metadata": {},
   "source": [
    "# 6. Automatic Speech Recognition (ASR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03298dd2-22f6-4259-a5a7-65e0228dbe74",
   "metadata": {},
   "source": [
    "**The `transcribe()` function takes an audio input and produces a transcription as a sequence of time-stamped text segments.**\n",
    "\n",
    "You can optionally provide a language code. It is one of **many optional settings**. Most of the other parameters control how the decoding is done. In the setup below, you can enable or disable Word-Level timestamps (`word_timestamps`). If enabled, per-word start/end times and word probabilities are added, but compute cost increases and timestamps can be less stable for short disfluencies or noisy speech. You can also enable or disable voice activity detection (`vad_filter`). If enabled, non-speech regions are removed, which typically speeds up transcription and can reduce hallucinations in silent/noisy parts, but it may also cut very short hesitation sounds (e.g., “um”, “ähm”) if they fall below the VAD sensitivity. Beam size (`beam_size`) controls how many alternative token sequences are considered during decoding. Larger values can slightly improve accuracy, but they also increase runtime. **For details on all available parameters and their defaults, refer to the [faster-whisper](https://github.com/SYSTRAN/faster-whisper) documentation.**\n",
    "\n",
    ">Example: \n",
    ">\n",
    ">In [noScribe](https://github.com/kaixxx/noScribe), hotwords from a separate file are passed into `transcribe()` via the `hotwords` parameter to implement the \"disfluencies\" on/off toggle in the noScribe application. In faster-whisper, these hotwords are inserted as extra prompt tokens before decoding, which slightly biases the decoder toward producing those words when the audio is uncertain (for example, very short filler sounds like \"uh\" or \"ehm\" that can be hard to distinguish from breathing or background noise). The original OpenAI [Whisper](https://github.com/openai/whisper) implementation does not provide a `hotwords` parameter under that name, so this behavior is specific to faster-whisper. The closest equivalent is providing a prompt via the `initial_prompt` parameter to bias decoding in a similar direction. \n",
    ">\n",
    ">If you want to find out how accounting for hesitation sounds could be implemented in the `transcribe()` function below, add to the parameters `hotwords = \"Äh, das ist, es ist, ähm, nicht so einfach.\"` or `hotwords = \"Uhm, okay, here's what I'm, like, thinking.\"` (taken from noScribe`s [prompt.yml](https://github.com/kaixxx/noScribe/blob/main/prompt.yml)) and run the transcription with the audio file \"C\" (german example) or \"D\" (english example) that you can select in Sect. 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70298242-ef91-4046-a019-956932ce1ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments, info = model.transcribe(\n",
    "    audio_16k,\n",
    "    language=\"de\",\n",
    "    word_timestamps=False, \n",
    "    vad_filter=True, \n",
    "    beam_size=5\n",
    ")\n",
    "\n",
    "segments = list(segments)  # The transcription will actually run here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d41380",
   "metadata": {},
   "source": [
    "**In faster-whisper transcription results, each text segment includes start and end time, the recognized text, the underlying token IDs, and several scores that can be inspected if needed.** These scores are model-internal confidence signals derived from the token probabilities during decoding. If word-level timestamps are enabled, each segment also contains a list of words with their own start and end times and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c43747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a817a8-e86f-4885-9f20-edb7ac658e9e",
   "metadata": {},
   "source": [
    "# 7. Speaker Diarization\n",
    "\n",
    "Next, we run the diarization model on the audio file. This gives us a **timeline of who speaks when**, for example: SPEAKER_00 speaks from 0–10 seconds, then SPEAKER_01 speaks from 10–15 seconds, and so on. By setting `min_speakers` and `max_speakers`, we constrain the output to exactly that number of speaker labels, even if the real audio might contain fewer or more speakers.\n",
    "\n",
    "In Sect. 4, `imageio_ffmpeg` provides an ffmpeg executable that we call directly to convert audio files. Pyannote, however, loads audio files via its own internal decoder (`TorchCodec`; see [pyannote.audio](https://github.com/pyannote/pyannote-audio)). This decoder can fail on some machines even when ffmpeg itself works. To avoid this, we pass audio \"from memory\": a waveform (the raw audio samples as a numeric array) plus the sample rate (samples per second). This makes the notebook more reliable on Windows and on JupyterHubs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1af5d-34d0-4e3b-8a82-105ec8dd5980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_for_pyannote(path):\n",
    "    waveform, sr = torchaudio.load(path)      # waveform shape: (channels, samples)\n",
    "    return {\"waveform\": waveform, \"sample_rate\": sr}\n",
    "\n",
    "diarization_result = diarization(\n",
    "    load_for_pyannote(audio_16k),\n",
    "    min_speakers=2,\n",
    "    max_speakers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c996dce-a3a3-4b42-89ca-a7652610a7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(diarization_result.speaker_diarization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d24bb8-6174-49b6-a12a-b329b89d4880",
   "metadata": {},
   "source": [
    "# 8. Merge Results\n",
    "\n",
    "We merge ASR output with diarization so that each spoken segment (and optionally each word) is linked to a speaker label (e.g., SPEAKER_00). If word-level timestamps are available, we assign speakers per word; otherwise we assign one speaker per segment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d469faf-3e54-48fb-90d8-faa22c9ff912",
   "metadata": {},
   "source": [
    "## 8.1 ASR Result Formating\n",
    "\n",
    "Since the output of faster-whisper is stored in its own custom Python objects, we first convert it into a simple **Python dictionary**. We define a function `to_whisper_result` that extracts only the fields we need. This makes the transcript easier to inspect, save, and process in later steps. Each segment gets a start time, an end time, the transcribed text, and (if word-level timestamps were enabled) a list of words with their own timing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b8cd3-85b7-4472-ba2a-1fc7bb7c9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_whisper_result(segments, language=None):\n",
    "    out = {\"segments\": []}\n",
    "    for s in segments:\n",
    "        item = {\n",
    "            \"start\": float(s.start),\n",
    "            \"end\": float(s.end),\n",
    "            \"text\": s.text or \"\",\n",
    "        }\n",
    "        if getattr(s, \"words\", None):\n",
    "            item[\"words\"] = [\n",
    "                {\n",
    "                    \"word\": w.word,\n",
    "                    \"start\": float(w.start),\n",
    "                    \"end\": float(w.end),\n",
    "                }\n",
    "                for w in s.words\n",
    "                if w.start is not None and w.end is not None\n",
    "            ]\n",
    "        out[\"segments\"].append(item)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba196bc-7fa4-416e-b04c-15ddd9baa428",
   "metadata": {},
   "source": [
    "Run conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050d8395-4a3d-485f-94d3-9ac82f53f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_result = to_whisper_result(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4590e0e-e1ee-43e1-ac54-032c3f48be55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(asr_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf850a-9e20-4308-87ea-64d432fbd16b",
   "metadata": {},
   "source": [
    "## 8.2 Align Results\n",
    "\n",
    "We combine the converted ASR output with the diarization output in the `align` function. For each word in the transcript, we determine which diarization time interval overlaps **the most** with that word, and assign the corresponding speaker label. We also assign a single main speaker to each ASR segment by choosing the speaker with the largest total word duration inside that segment.\n",
    "\n",
    "Diarization segments and ASR segments are independent time partitions, so they rarely match perfectly. The merge therefore compares time intervals and links them based on temporal overlap.\n",
    "\n",
    "If word-level timestamps are available, speaker labels are assigned per word. If not, one speaker is assigned to the entire ASR segment based on total overlap.\n",
    "\n",
    "The `overlap` flag is `Truev when two or more different speakers are active at any point within the interval.\n",
    "\n",
    "This step illustrates a simple manual merging strategy and can easily be adapted. For example, you could preserve overlaps explicitly, merge adjacent segments with the same speaker into longer turns, or change the decision rule (e.g., duration vs. word count). Because timestamps are short, any errors caused by overlaps remain limited to small time spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca2c4b-28cb-4d79-8b83-6c1adb7de41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(asr_result, diarization_result):\n",
    "    \"\"\"\n",
    "    Merge ASR output with diarization.\n",
    "    - If word timestamps exist: assign speaker per word and compute a main speaker per segment.\n",
    "    - Otherwise: assign one speaker per segment.\n",
    "    - overlap=True if >= 2 different speakers overlap the interval in time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert diarization result into a plain list to avoid iterating itertracks() many times\n",
    "    turns = []\n",
    "    for turn, _, spk in diarization_result.itertracks(yield_label=True):\n",
    "        turns.append((float(turn.start), float(turn.end), spk))\n",
    "\n",
    "    # We work with time intervals: [start, end] for ASR words/segments and [turn_start, turn_end] for diarization turns.\n",
    "    def overlap_seconds(s, e, ts, te):\n",
    "        \"\"\"Overlap duration in seconds between [s, e] and [ts, te].\"\"\"\n",
    "        return max(0.0, min(e, te) - max(s, ts))\n",
    "\n",
    "    def speakers_in_interval(s, e):\n",
    "        \"\"\"Return set of speakers that overlap [s, e].\"\"\"\n",
    "        spks = set()\n",
    "        for ts, te, spk in turns:\n",
    "            if overlap_seconds(s, e, ts, te) > 0:\n",
    "                spks.add(spk)\n",
    "        return spks\n",
    "\n",
    "    def best_speaker(s, e):\n",
    "        \"\"\"Return speaker with the largest total overlap with [s, e].\"\"\"\n",
    "        overlap_by_speaker = {}\n",
    "        for ts, te, spk in turns:\n",
    "            ov = overlap_seconds(s, e, ts, te)\n",
    "            if ov > 0:\n",
    "                overlap_by_speaker[spk] = overlap_by_speaker.get(spk, 0.0) + ov\n",
    "        return max(overlap_by_speaker, key=overlap_by_speaker.get) if overlap_by_speaker else None\n",
    "\n",
    "    def is_overlapped(s, e):\n",
    "        # Interval is 'overlapped' if >= 2 different speakers are active in it\n",
    "        return len(speakers_in_interval(s, e)) >= 2\n",
    "\n",
    "    # Define how final_result shall look like (structure for saving)\n",
    "    out = {\"segments\": []}\n",
    "\n",
    "    # Loop over each ASR segment\n",
    "    for seg in asr_result[\"segments\"]:\n",
    "\n",
    "        # 1. Add basic info: start/end time and full text\n",
    "        new_seg = {\n",
    "            \"start\": seg[\"start\"],\n",
    "            \"end\": seg[\"end\"],\n",
    "            \"text\": seg[\"text\"],\n",
    "        }\n",
    "\n",
    "        # 2. Mark whether this whole segment overlaps with other speech\n",
    "        new_seg[\"overlap\"] = is_overlapped(seg[\"start\"], seg[\"end\"])\n",
    "\n",
    "        # 3. Add word-level results in a list (if word_timestamps=True)\n",
    "        words = seg.get(\"words\", [])\n",
    "        if words:\n",
    "            new_words = []\n",
    "            dur_by_speaker = {}  # Total word duration per speaker within this ASR segment\n",
    "\n",
    "            for w in words:\n",
    "                # Assign speaker for this word based on maximum time overlap\n",
    "                spk = best_speaker(w[\"start\"], w[\"end\"])  # helper function defined above\n",
    "\n",
    "                # Copy word info and add the speaker label\n",
    "                wd = dict(w)\n",
    "                wd[\"speaker\"] = spk\n",
    "\n",
    "                # Mark whether this word lies in an overlap region\n",
    "                wd[\"overlap\"] = is_overlapped(w[\"start\"], w[\"end\"])\n",
    "                new_words.append(wd)\n",
    "\n",
    "                # Accumulate speaking time (word duration) per speaker\n",
    "                if spk is not None:\n",
    "                    dur_by_speaker[spk] = dur_by_speaker.get(spk, 0.0) + (w[\"end\"] - w[\"start\"])\n",
    "\n",
    "            # Save the word list inside the segment\n",
    "            new_seg[\"words\"] = new_words\n",
    "\n",
    "            # Segment-level speaker: pick the one with the most total word duration\n",
    "            new_seg[\"speaker\"] = max(dur_by_speaker, key=dur_by_speaker.get) if dur_by_speaker else None\n",
    "\n",
    "        else:\n",
    "            # If no word-level timestamps exist, assign a speaker for the entire segment directly\n",
    "            new_seg[\"speaker\"] = best_speaker(seg[\"start\"], seg[\"end\"])\n",
    "\n",
    "        # Add this processed segment to the output\n",
    "        out[\"segments\"].append(new_seg)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f050459-d392-4d89-8510-a64f6d7c1fcf",
   "metadata": {},
   "source": [
    "Run aligner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a51ad-c09e-476a-bcd0-be0283095bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = align(asr_result, diarization_result.speaker_diarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e97df-8c2d-4aa6-a0fe-073ce9f9cfe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265cf52-c955-4384-8782-1e7af40fd84f",
   "metadata": {},
   "source": [
    "# 9. Save Final Result\n",
    "\n",
    "Finally, we save a readable transcript as a text file: for each segment, we write out the speaker label, the transcribed text, and the segment’s start time. This produces a simple \"Speaker: text [time]\" format. This can easily be adapted depending on what is needed for further analysis inside Python or in external tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab62bb-cd1b-420e-bb9c-ab9f7f38e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as ...\n",
    "output_folder = \"../Results/\"\n",
    "txt_path = os.path.join(output_folder, f\"{file_name}.txt\")\n",
    "\n",
    "# Save \n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for seg in final_result[\"segments\"]:\n",
    "        speaker = seg.get(\"speaker\")\n",
    "        text = seg[\"text\"].strip()\n",
    "        start = str(timedelta(seconds=seg[\"start\"]))[:-3]\n",
    "        overlap_flag = \" [OVERLAP]\" if seg.get(\"overlap\") else \"\"\n",
    "        f.write(f\"{speaker}: {text}{overlap_flag} [{start}]\\n\\n\")\n",
    "\n",
    "print(\"Saved transcript to\", txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaba393",
   "metadata": {},
   "source": [
    "If you would only run Whisper transcription (without diarization), you can save the result directly like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as ...\n",
    "#output_folder = \"../Results/\"\n",
    "#os.makedirs(output_folder, exist_ok=True)\n",
    "#txt_path = os.path.join(output_folder, f\"{file_name}.txt\")\n",
    "\n",
    "# Save\n",
    "#with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#    for s in segments:\n",
    "#        start = str(timedelta(seconds=float(s.start)))[:-3]\n",
    "#        f.write(f\"[{start}] {s.text.strip()}\\n\")\n",
    "\n",
    "#print(\"Saved transcript to\", txt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperfw-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
